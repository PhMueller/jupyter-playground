{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow CAVE + HpBandster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will present you a example workflow of how to efficiently optimize a algorithm using our frameworks \n",
    "<a href=\"https://github.com/automl/CAVE\" target=\"_blank\">CAVE</a> and <a href=\"https://github.com/automl/HpBandSter\" target=\"_blank\">HpBandSter</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short introduction to the used frameworks\n",
    "### CAVE\n",
    "\n",
    "Hier sollte eine kurze Beschreibung von Cave stehen.\n",
    "\n",
    "### HpBandSter\n",
    "\n",
    "Modern deep learning methods are very sensitive to many hyperparameters, and, due to the long training times of state-of-the-art models, vanilla Bayesian hyperparameter optimization is typically computationally infeasible. On the other hand, bandit-based configuration evaluation approaches based on random search lack guidance and do not converge to the best configurations as quickly. With HpBanster, we propose to combine the benefits of both Bayesian optimization and bandit-based methods, in order to achieve the best of both worlds: strong anytime performance and fast convergence to optimal configurations. We propose a new practical state-of-the-art hyperparameter optimization method, which consistently outperforms both Bayesian optimization and Hyperband on a wide range of problem types, including high-dimensional toy functions, support vector machines, feed-forward neural networks, Bayesian neural networks, deep reinforcement learning, and convolutional neural networks. Our method is robust and versatile, while at the same time being conceptually simple and easy to implement.\n",
    "\n",
    "For more insights, please consider the paper: <a href=\"https://arxiv.org/abs/1807.01774\" target=\"_blank\">BOHB: Robust and Efficient Hyperparameter Optimization at Scale</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Workflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we guide through the following steps:\n",
    "#### 1) Given a algorithm to optimize and a *configuration space*, we will run BOHB on this problem. \n",
    "This will return us the optimization run results. For example the best hyperparameter configuration, which is often referred to as *incumbent*. \n",
    "Also a lot of information like which configurations has been used, as well as their performances.\n",
    "#### 2) We will pass the BOHB results into the CAVE-tool.\n",
    "It will give insights into the \n",
    "+ Parameter importance, \n",
    "+ performance analysis,\n",
    "+ feature analysis and \n",
    "+ configuration behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
